{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "In this notebook, we train a CNN to classify images from the CIFAR-10 database.\n",
    "\n",
    "The images in this database are small color images that fall into one of ten classes; some example images are pictured below.\n",
    "\n",
    "<img src='../../assets/cifar-10.png' width=50% height=50%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for [CUDA](http://pytorch.org/docs/stable/cuda.html)\n",
    "Since these are larger (32x32x3) images, it may prove useful to speed up your training time by using a GPU. CUDA is a parallel computing platform and CUDA Tensors are the same as typical Tensors, only they utilize GPU's for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if train_on_gpu:\n",
    "    print(\"Training on GPU device mode on.\")\n",
    "else:\n",
    "    print(\"Training on CPU device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data(http://pytorch.org/docs/stable/torchvision/datasets.html)\n",
    "Downloading may take a minute. We load in the training and test data, split the training data into a training and validation set, then create DataLoaders for each of these sets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets # sample available torchvision datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# define dataloader args\n",
    "num_workers = 2\n",
    "batch_size = 40\n",
    "val_set_ratio = 0.2\n",
    "\n",
    "# define afine image transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# choose the training and test dataset from Pytorch CIFAR\n",
    "train_set = datasets.CIFAR('data/', train=True, \n",
    "                           transform=transform, download=True)\n",
    "test_set = datasets.CIFAR('data/', train=False,\n",
    "                         transform=transform, download=True)\n",
    "\n",
    "# split train - validation set 80/20\n",
    "n_train = len(train_set)\n",
    "split_idx = int(np.floor(n_train * val_set_ratio))\n",
    "indices = list(range(n_train))\n",
    "np.random.shuffle(indices)\n",
    "train_idx, val_idx = indices[split_idx:], indices[:split_idx]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "# define data loader for train - val - test\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "val_loader = DataLoader(train_set, batch_size=batch_size, sampler=val_sampler, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "# specify image classes\n",
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\",\n",
    "           \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a Batch of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# helper function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "# display 20 images\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image size\n",
    "rgb_img = np.squeeze(images[0])\n",
    "n_channel = rgb_img.shape[0]\n",
    "rimg = rgb_img[0]\n",
    "cifar_size = rimg.shape[0] # shape: size x size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = np.squeeze(images[0])\n",
    "channels = ['red channel', 'green channel', 'blue channel']\n",
    "\n",
    "fig = plt.figure(figsize = (36, 36)) \n",
    "for idx in np.arange(rgb_img.shape[0]): #rgb\n",
    "    ax = fig.add_subplot(1, 3, idx + 1)\n",
    "    img = rgb_img[idx]\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(channels[idx])\n",
    "    width, height = img.shape\n",
    "    thresh = img.max()/2.5\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
    "            ax.annotate(str(val), xy=(y,x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center', size=8,\n",
    "                    color='white' if img[x][y]<thresh else 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Network Architecture\n",
    "This time, you'll define a CNN architecture. Instead of an MLP, which used linear, fully-connected layers, you'll use the following:\n",
    "\n",
    "Convolutional layers, which can be thought of as stack of filtered images.\n",
    "Maxpooling layers, which reduce the x-y size of an input, keeping only the most active pixels from the previous layer.\n",
    "The usual Linear + Dropout layers to avoid overfitting and produce a 10-dim output.\n",
    "A network with 2 convolutional layers is shown in the image below and in the code, and you've been given starter code with one convolutional and one maxpooling layer.\n",
    "\n",
    "<img src='../../assets/3-layer-conv.png' height=50% width=50% />\n",
    "\n",
    "The more convolutional layers you include, the more complex patterns in color and shape a model can detect. It's suggested that your final model include 2 or 3 convolutional layers as well as linear layers + dropout in between to avoid overfitting.\n",
    "\n",
    "It's good practice to look at existing research and implementations of related models as a starting point for defining your own models. You may find it useful to look at this PyTorch classification example or this, more complex Keras example to help decide on a final structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output volume for a convolutional layer\n",
    "\n",
    "\n",
    "We can compute the spatial size of the output volume as a function of the input volume size ($W$), the kernel/filter size ($F$), the stride with which they are applied ($S$), and the amount of zero padding used ($P$) on the border. The correct formula for calculating how many neurons define the output $W$ is given by $$W = {(W-F+2P)\\over S}+1 $$\n",
    "For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_size, out_class=10):\n",
    "\n",
    "        # if input_size is given in (s,s) format\n",
    "        if isinstance(input_size, (list,tuple)):\n",
    "            input_size = input_size[0]\n",
    "\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # 1st convolutional layer -> output size: (16 x in_size/2 x in_size/2) \n",
    "        self.conv2d_1 = nn.Sequential([\n",
    "            nn.Conv2d(3, 16, kernel_size=(3,3), padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "        ])\n",
    "        # 2nd convolutional layer -> output size: (32 x in_size/4 x in_size/4)\n",
    "        self.conv2d_2 = nn.Sequential([\n",
    "            nn.Conv2d(16, 32, kernel_size=(5,5), padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "        ])\n",
    "        \n",
    "        # flatten size for fc layer\n",
    "        flatten_size = 32*int(input_size/4)*int(input_size/4)\n",
    "        \n",
    "        # fully-connected layer, remember to flatten input\n",
    "        self.fc = nn.Linear(flatten_size, out_class)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # forward flow to convolutional layers\n",
    "        X = self.conv2d_1(X)\n",
    "        X = self.conv2d_2(X)\n",
    "        # flatten size\n",
    "        X = X.reshape(X.size(0), -1)\n",
    "        \n",
    "        return F.log_softmax(X, dim=1)\n",
    "\n",
    "model = SimpleCNN(input_size=size, out_class=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Specify Loss Function and Optimizer\n",
    "Decide on a loss and optimization function that is best suited for this classification task. The linked code examples from above, may be a good starting point; this PyTorch classification example or this, more complex Keras example. Pay close attention to the value for learning rate as this value determines how your model converges to a small error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
