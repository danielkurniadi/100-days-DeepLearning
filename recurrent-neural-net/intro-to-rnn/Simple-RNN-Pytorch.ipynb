{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN\n",
    "\n",
    "In ths notebook, we're going to train a simple RNN to do **time-series prediction**. Given some set of input data, it should be able to generate a prediction for the next time step!\n",
    "<img src='assets/time_prediction.png' width=40% />\n",
    "\n",
    "> * First, we'll create our data\n",
    "* Then, define an RNN in PyTorch\n",
    "* Finally, we'll train our network and see how it performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# visualization and plotting\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dummy time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# how many time steps/data pts are in one batch of data\n",
    "seq_length = 20\n",
    "\n",
    "# generate evenly spaced data pts\n",
    "time_steps = np.linspace(0, np.pi, seq_length + 1)\n",
    "data = np.sin(time_steps)\n",
    "data.resize((seq_length + 1, 1)) # size becomes (seq_length+1, 1). each sequence only has 1 feature\n",
    "\n",
    "x = data[:-1] # all but the last piece of data\n",
    "y = data[1:] # all but the first\n",
    "\n",
    "# display the data\n",
    "plt.plot(time_steps[1:], x, 'r.', label='input, x') # x\n",
    "plt.plot(time_steps[1:], y, 'b.', label='target, y') # y\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the RNN\n",
    "\n",
    "Next, we define an RNN in PyTorch. We'll use `nn.RNN` to create an RNN layer, then we'll add a last, fully-connected layer to get the output size that we want. An RNN takes in a number of parameters:\n",
    "* **input_size** - the number of expected features in the input x\n",
    "* **hidden_dim** - the number of features in the RNN output and in the hidden state\n",
    "* **n_layers** - the number of layers that make up the RNN, typically 1-3; greater than 1 means that you'll create a stacked RNN\n",
    "* **batch_first** - If True, then the input and output tensors are provided as (batch, seq, feature). Default input and output are of shape (seq_len, batch, input_size).\n",
    "* **dropout** - If non-zero and less than 1.0, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "* **bidirectional** â€“ If True, becomes a bidirectional LSTM. Default: False\n",
    "\n",
    "The outputs of RNN are `output` features and `h_n` hidden state.\n",
    "* **output**: tensor of shape `(seq_len, batch_size, num_dir * hidden_size)` containing the output features (h_t) from the last layer of the RNN, for each t. \n",
    "* **h_n**: tensor of shape `(num_layers * num_dir, batch, hidden_size)` containing the hidden state for t = seq_len.\n",
    "\n",
    "Take a look at the [RNN documentation](https://pytorch.org/docs/stable/nn.html#rnn) to read more about recurrent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    \"\"\"Simple RNN with a fully connected layer.\n",
    "        \n",
    "        Args:\n",
    "            * input_size - the number of expected features in the input x\n",
    "            * output_size - the number of expected feature in output x\n",
    "            * hidden_dim - the number of features in the RNN output and in the hidden state\n",
    "            * n_layers - the number of layers that make up the RNN, typically 1-3; greater than 1 means that you'll create a stacked RNN\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_dims, n_layers):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dims = hidden_dims # channel size of rnn output\n",
    "        \n",
    "        # initialise and define RNN configurations\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, \n",
    "                          n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x, h_n):\n",
    "        \"\"\"Feed-forward method.\n",
    "        \n",
    "            Args:\n",
    "                * x - input tensor of shape (batch_size, seq_length, input_size)\n",
    "                * h_n - tensor shape (num_layers, batch_size, hidden_size) containing hidden states\n",
    "        \"\"\"\n",
    "        r_out, h_n = self.rnn(x, h_n)\n",
    "        \n",
    "        # reshape r_out to be (batch_size * seq_length, hidden_dim)\n",
    "        r_out = r_out.view(-1, self.hidden_dim)\n",
    "        \n",
    "        output = self.fc(r_out)\n",
    "        return output, h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that dimensions are as expected\n",
    "test_rnn = RNN(input_size=1, output_size=1, hidden_dim=10, n_layers=2)\n",
    "\n",
    "# generate evenly spaced, test data pts\n",
    "time_steps = np.linspace(0, np.pi, seq_length)\n",
    "data = np.sin(time_steps)\n",
    "data.resize((seq_length, 1))\n",
    "\n",
    "test_input = torch.Tensor(data).unsqueeze(0) # give it a batch_size of 1 as first dimension\n",
    "print('Input size: ', test_input.size())\n",
    "\n",
    "# test out rnn sizes\n",
    "test_out, test_h = test_rnn(test_input, None)\n",
    "print('Output size: ', test_out.size())\n",
    "print('Hidden state size: ', test_h.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
