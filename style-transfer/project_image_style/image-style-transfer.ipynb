{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image style transfer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages/ Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image io\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "# visualisation/plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Style and Content Images\n",
    "You can load in any images you want!\n",
    "\n",
    "Additionally, it will be easier to have smaller images and to squish the content and style images so that they are of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "#HTTP: get -> BytesIO -> Image.open().convert('RGB')\n",
    "#local: Image.open().convert('RGB')\n",
    "\n",
    "# Resize(max_size) ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in VGG19 features\n",
    "VGG19 is split into two portions:\n",
    "\n",
    "- vgg19.features, which are all the convolutional and pooling layers\n",
    "- vgg19.classifier, which are the three linear, classifier layers at the end\n",
    "\n",
    "We only need the features portion, which we're going to load in and \"freeze\" the weights of, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg = models.vgg19(pretrained=True).features\n",
    "# for params in vgg: params.requires_grad(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print vgg layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## TODO: Complete mapping layer names of PyTorch's VGGNet to names from the paper\n",
    "    ## Need the layers for the content and style representations of an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content and Style Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed forward to vgg layers\n",
    "# use model._modules: dict<name, layer> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gram Matrix\n",
    "The output of every convolutional layer is a Tensor with dimensions associated with the batch_size, a depth, d and some height and width (h, w). The Gram matrix of a convolutional layer can be calculated as follows:\n",
    "\n",
    "- Get the depth, height, and width of a tensor using batch_size, d, h, w = tensor.size()\n",
    "- Reshape that tensor so that the spatial dimensions are flattened\n",
    "\n",
    "Calculate the gram matrix by multiplying the reshaped tensor by it's transpose\n",
    "Note: You can multiply two matrices using torch.mm(matrix1, matrix2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
